{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df34_old = pd.read_csv('to_analyze34.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>loudness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>205090</td>\n",
       "      <td>-10.059</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>109.928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>140526</td>\n",
       "      <td>-3.558</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>90.989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>199054</td>\n",
       "      <td>-4.410</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>114.044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>187541</td>\n",
       "      <td>-7.509</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>133.073</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>172325</td>\n",
       "      <td>-4.771</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>144.015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170648</th>\n",
       "      <td>1945</td>\n",
       "      <td>101427</td>\n",
       "      <td>-31.113</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>83.252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170649</th>\n",
       "      <td>1945</td>\n",
       "      <td>114900</td>\n",
       "      <td>-19.316</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>99.996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170650</th>\n",
       "      <td>1945</td>\n",
       "      <td>170910</td>\n",
       "      <td>-9.750</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>124.506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170651</th>\n",
       "      <td>1945</td>\n",
       "      <td>105778</td>\n",
       "      <td>-20.042</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>103.594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>1947</td>\n",
       "      <td>513040</td>\n",
       "      <td>-11.483</td>\n",
       "      <td>0.3430</td>\n",
       "      <td>75.008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170653 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  duration_ms  loudness  liveness    tempo  label\n",
       "0       2020       205090   -10.059    0.1130  109.928      1\n",
       "1       2020       140526    -3.558    0.2720   90.989      1\n",
       "2       2020       199054    -4.410    0.0936  114.044      1\n",
       "3       2020       187541    -7.509    0.0824  133.073      1\n",
       "4       2020       172325    -4.771    0.0931  144.015      1\n",
       "...      ...          ...       ...       ...      ...    ...\n",
       "170648  1945       101427   -31.113    0.0979   83.252      0\n",
       "170649  1945       114900   -19.316    0.1050   99.996      0\n",
       "170650  1945       170910    -9.750    0.2740  124.506      0\n",
       "170651  1945       105778   -20.042    0.1070  103.594      0\n",
       "170652  1947       513040   -11.483    0.3430   75.008      0\n",
       "\n",
       "[170653 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features to keep\n",
    "features_to_keep = ['year', 'duration_ms', 'loudness', 'liveness', 'tempo', 'label']\n",
    "\n",
    "# Drop all features except the ones to keep\n",
    "df34_clean = df34_old[features_to_keep]\n",
    "\n",
    "df34_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8618024933710318\n",
      "Test Accuracy: 0.7924174504116492\n",
      "Precision:  0.7924351542830377\n",
      "Recall:  0.7926672133067822\n",
      "F1:  0.7925511668081867\n",
      "Model Accuracy: 0.7924174504116492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = knn.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "print(\"Recall: \", recall)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"F1: \", f1)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN Model:\n",
      "Accuracy: 0.7984530192493627\n",
      "Precision: 0.7972823234385024\n",
      "Recall: 0.8006911092889774\n",
      "F1 Score: 0.7989830805645656\n"
     ]
    }
   ],
   "source": [
    "#tuned Knn\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_neighbors': [3, 5, 7],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'p': [1, 2]}\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn_tuned = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(knn_tuned, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new KNN classifier with the best hyperparameters\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred)\n",
    "precision_tuned = precision_score(y_test, y_pred)\n",
    "recall_tuned = recall_score(y_test, y_pred)\n",
    "f1_tuned = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the results for the tuned model\n",
    "print(\"Tuned KNN Model:\")\n",
    "print(\"Accuracy:\", accuracy_tuned)\n",
    "print(\"Precision:\", precision_tuned)\n",
    "print(\"Recall:\", recall_tuned)\n",
    "print(\"F1 Score:\", f1_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels for the matrix\n",
    "labels = ['Negative', 'Positive']\n",
    "\n",
    "# Create a heatmap plot of the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "# Set plot labels\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# Set plot title\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.696832744905583\n",
      "Test Accuracy: 0.6965515220767045\n",
      "Model Accuracy: 0.6967764996806385\n",
      "Precision: 0.6806519283524286\n",
      "Recall: 0.741126859552536\n",
      "F1 Score: 0.7096032524884341\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "overall_accuracy = accuracy_score(y, logreg.predict(X))\n",
    "print(\"Model Accuracy:\", overall_accuracy)\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.64218341        nan 0.64218341        nan 0.64218341]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Model:\n",
      "Accuracy: 0.6965515220767045\n",
      "Precision: 0.6806519283524286\n",
      "Recall: 0.741126859552536\n",
      "F1 Score: 0.7096032524884341\n"
     ]
    }
   ],
   "source": [
    "#tuned LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1.0, 10.0],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new Logistic Regression classifier with the best hyperparameters\n",
    "best_logreg = LogisticRegression(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_logreg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Tuned Logistic Regression Model:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8053675544226656\n",
      "Precision: 0.8081172091924145\n",
      "Recall: 0.8011596579594705\n",
      "F1 Score: 0.804623393429605\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8433096012422724\n",
      "Precision: 0.8501971090670171\n",
      "Recall: 0.8336652219749326\n",
      "F1 Score: 0.84185001182872\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\elits\\Documents\\GitHub\\final\\hope_it_works\\Models_top5_features.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elits/Documents/GitHub/final/hope_it_works/Models_top5_features.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Perform randomized search to find the best hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elits/Documents/GitHub/final/hope_it_works/Models_top5_features.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m random_search \u001b[39m=\u001b[39m RandomizedSearchCV(rf_classifier, param_distributions\u001b[39m=\u001b[39mparam_dist, n_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/elits/Documents/GitHub/final/hope_it_works/Models_top5_features.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m random_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elits/Documents/GitHub/final/hope_it_works/Models_top5_features.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elits/Documents/GitHub/final/hope_it_works/Models_top5_features.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m best_params \u001b[39m=\u001b[39m random_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1766\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1765\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1766\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1767\u001b[0m         ParameterSampler(\n\u001b[0;32m   1768\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1769\u001b[0m         )\n\u001b[0;32m   1770\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m         clone(base_estimator),\n\u001b[0;32m    841\u001b[0m         X,\n\u001b[0;32m    842\u001b[0m         y,\n\u001b[0;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    849\u001b[0m     )\n\u001b[0;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    852\u001b[0m     )\n\u001b[0;32m    853\u001b[0m )\n\u001b[0;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    451\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    452\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    453\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    454\u001b[0m )(\n\u001b[0;32m    455\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    456\u001b[0m         t,\n\u001b[0;32m    457\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    458\u001b[0m         X,\n\u001b[0;32m    459\u001b[0m         y,\n\u001b[0;32m    460\u001b[0m         sample_weight,\n\u001b[0;32m    461\u001b[0m         i,\n\u001b[0;32m    462\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    463\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    464\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    465\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    468\u001b[0m )\n\u001b[0;32m    470\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    938\u001b[0m         X,\n\u001b[0;32m    939\u001b[0m         y,\n\u001b[0;32m    940\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    941\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    942\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {'n_estimators': randint(100, 1000),\n",
    "              'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "              'min_samples_split': randint(2, 20),\n",
    "              'min_samples_leaf': randint(1, 10),\n",
    "              'max_features': ['auto', 'sqrt', 'log2', None]}\n",
    "\n",
    "# Perform randomized search to find the best hyperparameters\n",
    "random_search = RandomizedSearchCV(rf_classifier, param_distributions=param_dist, n_iter=10, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Create a new Random Forest classifier with the best hyperparameters\n",
    "best_rf_classifier = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "best_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
