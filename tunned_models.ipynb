{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from 'to_analyze34.csv'\n",
    "df34 = pd.read_csv('to_analyze34.csv')\n",
    "\n",
    "# Keep only the specified features\n",
    "df34_clean = df34.loc[:, ['liveness', 'acousticness', 'energy', 'danceability', 'valence', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liveness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>energy</th>\n",
       "      <th>danceability</th>\n",
       "      <th>valence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.7370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.4540</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.3570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170648</th>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170649</th>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170650</th>\n",
       "      <td>0.2740</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.4440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170651</th>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>0.3430</td>\n",
       "      <td>0.7660</td>\n",
       "      <td>0.5740</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170653 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        liveness  acousticness  energy  danceability  valence  label\n",
       "0         0.1130        0.4010  0.5730         0.731   0.1450      1\n",
       "1         0.2720        0.2210  0.7220         0.700   0.7560      1\n",
       "2         0.0936        0.0112  0.7650         0.746   0.7370      1\n",
       "3         0.0824        0.0194  0.4540         0.935   0.3570      1\n",
       "4         0.0931        0.4680  0.8020         0.737   0.6820      1\n",
       "...          ...           ...     ...           ...      ...    ...\n",
       "170648    0.0979        0.9920  0.0082         0.224   0.0696      0\n",
       "170649    0.1050        0.1530  0.1760         0.700   0.4910      0\n",
       "170650    0.2740        0.9770  0.2540         0.517   0.4440      0\n",
       "170651    0.1070        0.1610  0.2380         0.699   0.6120      0\n",
       "170652    0.3430        0.7660  0.5740         0.234   0.1440      0\n",
       "\n",
       "[170653 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df34_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.809005142028391\n",
      "Test Accuracy: 0.7289853798599514\n",
      "Precision:  0.7245436804040868\n",
      "Recall:  0.7393112334543751\n",
      "F1:  0.7318529684601113\n",
      "Model Accuracy: 0.7289853798599514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = knn.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "print(\"Recall: \", recall)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"F1: \", f1)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN Model:\n",
      "Accuracy: 0.7360757083003721\n",
      "Precision: 0.7308528906697195\n",
      "Recall: 0.7478036781070634\n",
      "F1 Score: 0.7392311255210746\n"
     ]
    }
   ],
   "source": [
    "#tuned Knn\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_neighbors': [3, 5, 7],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'p': [1, 2]}\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn_tuned = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(knn_tuned, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new KNN classifier with the best hyperparameters\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred)\n",
    "precision_tuned = precision_score(y_test, y_pred)\n",
    "recall_tuned = recall_score(y_test, y_pred)\n",
    "f1_tuned = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the results for the tuned model\n",
    "print(\"Tuned KNN Model:\")\n",
    "print(\"Accuracy:\", accuracy_tuned)\n",
    "print(\"Precision:\", precision_tuned)\n",
    "print(\"Recall:\", recall_tuned)\n",
    "print(\"F1 Score:\", f1_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "\n",
    "# Add labels, title, and axis ticks\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "plt.text(0.5, -0.2, f\"Accuracy: {accuracy:.2f}\", ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7433820190152503\n",
      "Test Accuracy: 0.7431074389850869\n",
      "Model Accuracy: 0.7433271023656192\n",
      "Precision: 0.7505127277114247\n",
      "Recall: 0.7287103197844677\n",
      "F1 Score: 0.7394508498751932\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "overall_accuracy = accuracy_score(y, logreg.predict(X))\n",
    "print(\"Model Accuracy:\", overall_accuracy)\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\elits\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.74338936        nan 0.74336739        nan 0.74336007]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Model:\n",
      "Accuracy: 0.742931645717969\n",
      "Precision: 0.7502411963338157\n",
      "Recall: 0.7287103197844677\n",
      "F1 Score: 0.7393190326222592\n"
     ]
    }
   ],
   "source": [
    "#tuned LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1.0, 10.0],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new Logistic Regression classifier with the best hyperparameters\n",
    "best_logreg = LogisticRegression(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_logreg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Tuned Logistic Regression Model:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6754270311447071\n",
      "Precision: 0.6777751423149905\n",
      "Recall: 0.6694389129670845\n",
      "F1 Score: 0.6735812363722081\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7547097946148662\n",
      "Precision: 0.7505759041695462\n",
      "Recall: 0.7633243528171488\n",
      "F1 Score: 0.7568964515941693\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.699862295274091\n",
      "Precision: 0.6942990441511152\n",
      "Recall: 0.7147124282534848\n",
      "F1 Score: 0.7043578643578644\n"
     ]
    }
   ],
   "source": [
    "#tuned randomforest\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)  # Set the number of components to retain\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['auto']\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new Random Forest classifier with the best hyperparameters\n",
    "best_rf_classifier = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "best_rf_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_rf_classifier.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\elits\\anaconda3\\lib\\site-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\elits\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\elits\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7592804195599309\n",
      "Precision: 0.753317318691375\n",
      "Recall: 0.7714068173831556\n",
      "F1 Score: 0.7622547601134324\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Train the model\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73349740704931\n",
      "Precision: 0.7167699163134442\n",
      "Recall: 0.7725196204755769\n",
      "F1 Score: 0.7436013079264855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the further reduced hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [3]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(gb_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new Gradient Boosting classifier with the best hyperparameters\n",
    "best_gb_classifier = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "best_gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_gb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7566435205531629\n",
      "Precision: 0.7483289905970318\n",
      "Recall: 0.7737495607356214\n",
      "F1 Score: 0.7608269983874683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_clean.drop('label', axis=1)\n",
    "y = df34_clean['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reduce the training dataset size\n",
    "X_train_small, _, y_train_small, _ = train_test_split(X_train, y_train, train_size=0.5, random_state=42)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(X_train_small)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "svm_classifier.fit(X_train_pca, y_train_small)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_classifier.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7309855853744287\n",
      "Precision: 0.7215638498172555\n",
      "Recall: 0.7582635009310987\n",
      "F1 Score: 0.7394586005334545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce the dataset size\n",
    "df34_small = df34_clean.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df34_small.drop('label', axis=1)\n",
    "y = df34_small['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Define the hyperparameter grid with a single combination\n",
    "param_grid = {\n",
    "    'C': [1],\n",
    "    'gamma': ['scale'],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new SVM classifier with the best hyperparameters\n",
    "best_svm_classifier = SVC(**best_params)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "best_svm_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = best_svm_classifier.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
