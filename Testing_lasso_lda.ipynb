{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df34_old = pd.read_csv('to_analyze34.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_billboard</th>\n",
       "      <th>liveness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>energy</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.0544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.0993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.4540</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170648</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170649</th>\n",
       "      <td>False</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.9120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170650</th>\n",
       "      <td>False</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170651</th>\n",
       "      <td>False</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>False</td>\n",
       "      <td>0.3430</td>\n",
       "      <td>0.7660</td>\n",
       "      <td>0.5740</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170653 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_billboard  liveness  acousticness  energy  speechiness  label\n",
       "0              False    0.1130        0.4010  0.5730       0.0544      1\n",
       "1              False    0.2720        0.2210  0.7220       0.0369      1\n",
       "2              False    0.0936        0.0112  0.7650       0.0993      1\n",
       "3              False    0.0824        0.0194  0.4540       0.3750      1\n",
       "4              False    0.0931        0.4680  0.8020       0.0878      1\n",
       "...              ...       ...           ...     ...          ...    ...\n",
       "170648         False    0.0979        0.9920  0.0082       0.0350      0\n",
       "170649         False    0.1050        0.1530  0.1760       0.9120      0\n",
       "170650         False    0.2740        0.9770  0.2540       0.1200      0\n",
       "170651         False    0.1070        0.1610  0.2380       0.9130      0\n",
       "170652         False    0.3430        0.7660  0.5740       0.0481      0\n",
       "\n",
       "[170653 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features to keep\n",
    "features_to_keep = ['is_billboard', 'liveness', 'acousticness', 'energy', 'speechiness', 'label']\n",
    "\n",
    "# Drop all features except the ones to keep\n",
    "df34_lda = df34_old[features_to_keep]\n",
    "\n",
    "df34_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8023249000161147\n",
      "Test Accuracy: 0.7155664938032873\n",
      "Precision:  0.7045429301343996\n",
      "Recall:  0.7430010542345086\n",
      "F1:  0.7232611174458381\n",
      "Model Accuracy: 0.7155664938032873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_lda.drop('label', axis=1)\n",
    "y = df34_lda['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = knn.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "print(\"Recall: \", recall)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"F1: \", f1)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7407084572449861\n",
      "Test Accuracy: 0.7398845624212592\n",
      "Model Accuracy: 0.7405436763490827\n",
      "Precision: 0.7455951096727795\n",
      "Recall: 0.728651751200656\n",
      "F1 Score: 0.7370260663507109\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df34_lda.drop('label', axis=1)\n",
    "y = df34_lda['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "overall_accuracy = accuracy_score(y, logreg.predict(X))\n",
    "print(\"Model Accuracy:\", overall_accuracy)\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_billboard</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>explicit</th>\n",
       "      <th>valence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.4540</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170648</th>\n",
       "      <td>False</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170649</th>\n",
       "      <td>False</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170650</th>\n",
       "      <td>False</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170651</th>\n",
       "      <td>False</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>False</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.5740</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170653 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_billboard  danceability  energy  explicit  valence  label\n",
       "0              False         0.731  0.5730         1   0.1450      1\n",
       "1              False         0.700  0.7220         1   0.7560      1\n",
       "2              False         0.746  0.7650         0   0.7370      1\n",
       "3              False         0.935  0.4540         1   0.3570      1\n",
       "4              False         0.737  0.8020         1   0.6820      1\n",
       "...              ...           ...     ...       ...      ...    ...\n",
       "170648         False         0.224  0.0082         0   0.0696      0\n",
       "170649         False         0.700  0.1760         1   0.4910      0\n",
       "170650         False         0.517  0.2540         0   0.4440      0\n",
       "170651         False         0.699  0.2380         1   0.6120      0\n",
       "170652         False         0.234  0.5740         0   0.1440      0\n",
       "\n",
       "[170653 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df50_old = pd.read_csv('to_analyze50.csv')\n",
    "\n",
    "# Define the features to keep\n",
    "features_to_keep = ['is_billboard', 'danceability', 'energy', 'explicit', 'valence', 'label']\n",
    "\n",
    "# Drop all features except the ones to keep\n",
    "df50_lda = df50_old[features_to_keep]\n",
    "\n",
    "df50_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8359238804002286\n",
      "Test Accuracy: 0.779350150889221\n",
      "Precision:  0.5097854723372224\n",
      "Recall:  0.35481335952848725\n",
      "F1:  0.41841068808402193\n",
      "Model Accuracy: 0.779350150889221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df50_lda.drop('label', axis=1)\n",
    "y = df50_lda['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = knn.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "print(\"Recall: \", recall)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"F1: \", f1)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7878656919763848\n",
      "Test Accuracy: 0.7907767132518825\n",
      "Model Accuracy: 0.7884479030547368\n",
      "Precision: 0.5741296518607443\n",
      "Recall: 0.2505566470203012\n",
      "F1 Score: 0.34886477614662165\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df50_lda.drop('label', axis=1)\n",
    "y = df50_lda['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "overall_accuracy = accuracy_score(y, logreg.predict(X))\n",
    "print(\"Model Accuracy:\", overall_accuracy)\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explicit</th>\n",
       "      <th>is_billboard</th>\n",
       "      <th>year</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>3.418167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>2.342100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>3.317567</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>3.125683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>2.872083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170648</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1945</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>1.690450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170649</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1945</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>1.915000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170650</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1945</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>2.848500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170651</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1945</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>1.762967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1947</td>\n",
       "      <td>0.7660</td>\n",
       "      <td>8.550667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170653 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        explicit  is_billboard  year  acousticness  duration_min  label\n",
       "0              1         False  2020        0.4010      3.418167      1\n",
       "1              1         False  2020        0.2210      2.342100      1\n",
       "2              0         False  2020        0.0112      3.317567      1\n",
       "3              1         False  2020        0.0194      3.125683      1\n",
       "4              1         False  2020        0.4680      2.872083      1\n",
       "...          ...           ...   ...           ...           ...    ...\n",
       "170648         0         False  1945        0.9920      1.690450      0\n",
       "170649         1         False  1945        0.1530      1.915000      0\n",
       "170650         0         False  1945        0.9770      2.848500      0\n",
       "170651         1         False  1945        0.1610      1.762967      0\n",
       "170652         0         False  1947        0.7660      8.550667      0\n",
       "\n",
       "[170653 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features to keep\n",
    "features_to_keep = ['explicit', 'is_billboard', 'year', 'acousticness', 'duration_min', 'label']\n",
    "\n",
    "# Drop all features except the ones to keep\n",
    "df50_lasso = df50_old[features_to_keep]\n",
    "\n",
    "df50_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8897100833565286\n",
      "Test Accuracy: 0.8509566083618998\n",
      "Precision:  0.7008196721311475\n",
      "Recall:  0.5823182711198428\n",
      "F1:  0.6360970026468273\n",
      "Model Accuracy: 0.8509566083618998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df50_lasso.drop('label', axis=1)\n",
    "y = df50_lasso['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = knn.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "print(\"Precision: \", precision)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "print(\"Recall: \", recall)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"F1: \", f1)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.780482266594395\n",
      "Test Accuracy: 0.7857373062611702\n",
      "Model Accuracy: 0.7815332868452357\n",
      "Precision: 0.5596738324684952\n",
      "Recall: 0.19777341191879502\n",
      "F1 Score: 0.2922674924997581\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df50_lasso.drop('label', axis=1)\n",
    "y = df50_lasso['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the Logistic Regression model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the training set\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model for the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Calculate the accuracy of the model for the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for the test set\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "overall_accuracy = accuracy_score(y, logreg.predict(X))\n",
    "print(\"Model Accuracy:\", overall_accuracy)\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
